{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "# Principal Component Analysis (PCA) is by far the most popular dimensionality reduction algorithm.\n",
    "# First it identifies the hyperplane that lies closest to the data, and then it projects the data onto it\n",
    "\n",
    "# So how can you find the principal components of a training set? Luckily, there is a standard matrix\n",
    "# factorization technique called Singular Value Decomposition (SVD) that can decompose the training set\n",
    "# matrix X into the dot product of three matrices U · Σ · V T , where V T contains all the principal components\n",
    "# that we are looking for\n",
    "\n",
    "# The following Python code uses NumPy’s svd() function to obtain all the principal components of the\n",
    "# training set, then extracts the first two PCs:\n",
    "X_centered = X - X.mean(axis=0)\n",
    "U, s, V = np.linalg.svd(X_centered)\n",
    "c1 = V.T[:, 0]\n",
    "c2 = V.T[:, 1]\n",
    "# The following Python code projects the training set onto the plane defined by the first two principal\n",
    "# components:\n",
    "W2 = V.T[:, :2]\n",
    "X2D = X_centered.dot(W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Scikit-Learn\n",
    "# Scikit-Learn’s PCA class implements PCA using SVD decomposition just like we did before. The\n",
    "# following code applies PCA to reduce the dimensionality of the dataset down to two dimensions (note\n",
    "# that it automatically takes care of centering the data):\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 2)\n",
    "X2D = pca.fit_transform(X)\n",
    "# After fitting the PCA transformer to the dataset, you can access the principal components using the\n",
    "# components_ variable (note that it contains the PCs as horizontal vectors, so, for example, the first\n",
    "# principal component is equal to pca.components_.T[:, 0] )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another very useful piece of information is the explained variance ratio of each principal component,\n",
    "# available via the explained_variance_ratio_ variable.\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of arbitrarily choosing the number of dimensions to reduce down to, it is generally preferable to\n",
    "# choose the number of dimensions that add up to a sufficiently large portion of the variance (e.g., 95%).\n",
    "# Unless, of course, you are reducing dimensionality for data visualization — in that case you will\n",
    "# generally want to reduce the dimensionality down to 2 or 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code computes PCA without reducing dimensionality, then computes the minimum number\n",
    "# of dimensions required to preserve 95% of the training set’s variance:\n",
    "pca = PCA()\n",
    "pca.fit(X_train)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "d = np.argmax(cumsum >= 0.95) + 1\n",
    "# You could then set n_components=d and run PCA again. However, there is a much better option: instead\n",
    "# of specifying the number of principal components you want to preserve, you can set n_components to be\n",
    "# a float between 0.0 and 1.0 , indicating the ratio of variance you wish to preserve:\n",
    "pca = PCA(n_components=0.95)\n",
    "X_reduced = pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incremental PCA\n",
    "\n",
    "\n",
    "# One problem with the preceding implementation of PCA is that it requires the whole training set to fit in\n",
    "# memory in order for the SVD algorithm to run. Fortunately, Incremental PCA (IPCA) algorithms have\n",
    "# been developed: you can split the training set into mini-batches and feed an IPCA algorithm one mini-\n",
    "# batch at a time. This is useful for large training sets, and also to apply PCA online (i.e., on the fly, as new\n",
    "# instances arrive).\n",
    "\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "n_batches = 100\n",
    "inc_pca = IncrementalPCA(n_components=154)\n",
    "for X_batch in np.array_split(X_train, n_batches):\n",
    "inc_pca.partial_fit(X_batch)\n",
    "\n",
    "\n",
    "# Alternatively, you can use NumPy’s memmap class, which allows you to manipulate a large array stored in\n",
    "# a binary file on disk as if it were entirely in memory; the class loads only the data it needs in memory,\n",
    "# when it needs it.\n",
    "# Since the IncrementalPCA class uses only a small part of the array at any given time,\n",
    "# the memory usage remains under control. This makes it possible to call the usual fit() method, as you\n",
    "# can see in the following code:\n",
    "X_mm = np.memmap(filename, dtype=\"float32\", mode=\"readonly\", shape=(m, n))\n",
    "batch_size = m // n_batches\n",
    "inc_pca = IncrementalPCA(n_components=154, batch_size=batch_size)\n",
    "inc_pca.fit(X_mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomized PCA\n",
    "\n",
    "\n",
    "# Scikit-Learn offers yet another option to perform PCA, called Randomized PCA. This is a stochastic\n",
    "# algorithm that quickly finds an approximation of the first d principal components. Its computational\n",
    "# complexity is O(m × d 2 ) + O(d 3 ), instead of O(m × n 2 ) + O(n 3 ), so it is dramatically faster than the\n",
    "# previous algorithms when d is much smaller than n.\n",
    "rnd_pca = PCA(n_components=154, svd_solver=\"randomized\")\n",
    "X_reduced = rnd_pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KERNEL PCA\n",
    "# It turns out that the same trick can be applied to PCA, making it possible to perform complex nonlinear\n",
    "# projections for dimensionality reduction. This is called Kernel PCA (kPCA). 6 It is often good at\n",
    "# preserving clusters of instances after projection, or sometimes even unrolling datasets that lie close to a\n",
    "# twisted manifold.\n",
    "\n",
    "\n",
    "from sklearn.decomposition import KernelPCA\n",
    "rbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.04)\n",
    "X_reduced = rbf_pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to select a kernel\n",
    "# This decision can be made by finding the pre-image reconstruction error\n",
    "# It can be done this way\n",
    "\n",
    "# Scikit-Learn will do this automatically if you set fit_inverse_transform=True , as shown in the\n",
    "# following code: 7\n",
    "rbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.0433,\n",
    "fit_inverse_transform=True)\n",
    "X_reduced = rbf_pca.fit_transform(X)\n",
    "X_preimage = rbf_pca.inverse_transform(X_reduced)\n",
    "\n",
    "\n",
    "You can then compute the reconstruction pre-image error:\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(X, X_preimage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLE\n",
    "\n",
    "# Locally Linear Embedding (LLE) 8 is another very powerful nonlinear dimensionality reduction\n",
    "# (NLDR) technique. It is a Manifold Learning technique that does not rely on projections like the previous\n",
    "# algorithms. In a nutshell, LLE works by first measuring how each training instance linearly relates to its\n",
    "# closest neighbors (c.n.), and then looking for a low-dimensional representation of the training set where\n",
    "# these local relationships are best preserved (more details shortly). This makes it particularly good at\n",
    "# unrolling twisted manifolds, especially when there is not too much noise.\n",
    "\n",
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10)\n",
    "X_reduced = lle.fit_transform(X)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

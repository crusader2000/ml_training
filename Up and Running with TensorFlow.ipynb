{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "# The following code creates the graph\n",
    "import tensorflow as tf\n",
    "x = tf.Variable(3, name=\"x\")\n",
    "y = tf.Variable(4, name=\"y\")\n",
    "f = x*x*y + y + 2\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    x.initializer.run()\n",
    "    y.initializer.run()\n",
    "    result = f.eval()\n",
    "print(result)\n",
    "# Calling x.initializer.run() is equivalent to calling tf.get_default_session().run(x.initializer) , and similarly f.eval() is\n",
    "# equivalent to calling tf.get_default_session().run(f) . This makes the code easier to read.\n",
    "# Moreover, the session is automatically closed at the end of the block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot evaluate tensor using `eval()`: No default session is registered. Use `with sess.as_default()` or pass an explicit session to `eval(session=sess)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-d0c61b64918a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# actually initialize all the variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;31m# Inside Jupyter or within a Python shell you may prefer to create an InteractiveSession . The only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# difference from a regular Session is that when an InteractiveSession is created it automatically setsitself as the default session, so you don’t need a with block (but you do need to close the session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/ml/handson/env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m     \"\"\"\n\u001b[0;32m--> 581\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/ml/handson/env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   3781\u001b[0m     \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3782\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msession\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3783\u001b[0;31m       raise ValueError(\"Cannot evaluate tensor using `eval()`: No default \"\n\u001b[0m\u001b[1;32m   3784\u001b[0m                        \u001b[0;34m\"session is registered. Use `with \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3785\u001b[0m                        \u001b[0;34m\"sess.as_default()` or pass an explicit session to \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot evaluate tensor using `eval()`: No default session is registered. Use `with sess.as_default()` or pass an explicit session to `eval(session=sess)`"
     ]
    }
   ],
   "source": [
    "# Instead of manually running the initializer for every single variable, you can use the\n",
    "# global_variables_initializer() function. Note that it does not actually perform the initialization\n",
    "# immediately, but rather creates a node in the graph that will initialize all variables when it is run:\n",
    "init = tf.global_variables_initializer()\n",
    "# prepare an init node\n",
    "with tf.Session() as sess:\n",
    "    init.run() # actually initialize all the variables\n",
    "result = f.eval()\n",
    "# Inside Jupyter or within a Python shell you may prefer to create an InteractiveSession . The only\n",
    "# difference from a regular Session is that when an InteractiveSession is created it automatically setsitself as the default session, so you don’t need a with block (but you do need to close the session\n",
    "# manually when you are done with it):\n",
    "sess = tf.InteractiveSession()\n",
    "init.run()\n",
    "result = f.eval()\n",
    "print(result)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.7465141e+01]\n",
      " [ 4.3573415e-01]\n",
      " [ 9.3382923e-03]\n",
      " [-1.0662201e-01]\n",
      " [ 6.4410698e-01]\n",
      " [-4.2513184e-06]\n",
      " [-3.7732250e-03]\n",
      " [-4.2664889e-01]\n",
      " [-4.4051403e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression with TensorFlow\n",
    "# In the examples so far, the tensors just contained a single scalar value, but you can of course perform\n",
    "# computations on arrays of any shape. For example, the following code manipulates 2D arrays to perform\n",
    "# Linear Regression on the California housing dataset (introduced in Chapter 2). It starts by fetching the\n",
    "# dataset; then it adds an extra bias input feature (x 0 = 1) to all training instances (it does so using NumPy so\n",
    "# it runs immediately); then it creates two TensorFlow constant nodes, X and y , to hold this data and the\n",
    "# targets, 4 and it uses some of the matrix operations provided by TensorFlow to define theta .\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "m, n = housing.data.shape\n",
    "housing_data_plus_bias = np.c_[np.ones((m, 1)), housing.data]\n",
    "\n",
    "X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "XT = tf.transpose(X)\n",
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    theta_value = theta.eval()\n",
    "    print(theta_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 2215843.2\n",
      "Epoch 100 MSE = nan\n",
      "Epoch 200 MSE = nan\n",
      "Epoch 300 MSE = nan\n",
      "Epoch 400 MSE = nan\n",
      "Epoch 500 MSE = nan\n",
      "Epoch 600 MSE = nan\n",
      "Epoch 700 MSE = nan\n",
      "Epoch 800 MSE = nan\n",
      "Epoch 900 MSE = nan\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Implementing Gradient Descent\n",
    "# When using Gradient Descent, remember that it is important to first normalize the input feature vectors, or else training may be\n",
    "# much slower. You can do this using TensorFlow, NumPy, Scikit-Learn’s StandardScaler , or any other solution you prefer. The\n",
    "# following code assumes that this normalization has already been done.\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    best_theta = theta.eval()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "#     , TensorFlow’s autodiff feature comes to the rescue: it can automatically and efficiently\n",
    "# compute the gradients for you. Simply replace the gradients = ... line in the Gradient Descent code in\n",
    "# the previous section with the following line, and the code will continue to work just fine:\n",
    "\n",
    "\n",
    "gradients = tf.gradients(mse, [theta])[0]\n",
    "\n",
    "# So TensorFlow computes the gradients for you. But it gets even easier: it also provides a number of\n",
    "# optimizers out of the box, including a Gradient Descent optimizer. You can simply replace the preceding\n",
    "# gradients = ... and training_op = ... lines with the following code, and once again everything\n",
    "# will just work fine:\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "\n",
    "# If you want to use a different type of optimizer, you just need to change one line. For example, you can use\n",
    "# a momentum optimizer (which often converges much faster than Gradient Descent; see Chapter 11) by\n",
    "# defining the optimizer like this:\n",
    "\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,\n",
    "momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feeding Data to the Training Algorithm\n",
    "\n",
    "\n",
    "# Let’s try to modify the previous code to implement Mini-batch Gradient Descent. For this, we need a way\n",
    "# to replace X and y at every iteration with the next mini-batch. The simplest way to do this is to use\n",
    "# placeholder nodes. These nodes are special because they don’t actually perform any computation, they\n",
    "# just output the data you tell them to output at runtime. They are typically used to pass the training data to\n",
    "# TensorFlow during training. If you don’t specify a value at runtime for a placeholder, you get an\n",
    "# exception.\n",
    "# To create a placeholder node, you must call the placeholder() function and specify the output tensor’s\n",
    "# data type. Optionally, you can also specify its shape, if you want to enforce it. If you specify None for a\n",
    "# dimension, it means “any size.” For example, the following code creates a placeholder node A , and also a\n",
    "# node B = A + 5 . When we evaluate B , we pass a feed_dict to the eval() method that specifies the\n",
    "# value of A . Note that A must have rank 2 (i.e., it must be two-dimensional) and there must be three columns\n",
    "# (or else an exception is raised), but it can have any number of rows.\n",
    "\n",
    "A = tf.placeholder(tf.float32, shape=(None, 3))\n",
    "B = A + 5\n",
    "with tf.Session() as sess:\n",
    "    B_val_1 = B.eval(feed_dict={A: [[1, 2, 3]]})\n",
    "    B_val_2 = B.eval(feed_dict={A: [[4, 5, 6], [7, 8, 9]]})\n",
    "print(B_val_1)\n",
    "# [[ 6. 7. 8.]]\n",
    "print(B_val_2)\n",
    "# [[ 9. 10. 11.]\n",
    "# [ 12. 13. 14.]]\n",
    "# To implement Mini-batch Gradient Descent, we only need to tweak the existing code slightly. First change\n",
    "# the definition of X and y in the construction phase to make them placeholder nodes:\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "# Then define the batch size and compute the total number of batches:\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "# Finally, in the execution phase, fetch the mini-batches one by one, then provide the value of X and y via\n",
    "# the feed_dict parameter when evaluating a node that depends on either of them.\n",
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    [...] # load the data from disk\n",
    "    return X_batch, y_batch\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "    best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow makes saving and restoring a model very easy. Just create a Saver node at the end of the\n",
    "# construction phase (after all variable nodes are created); then, in the execution phase, just call its save()\n",
    "# method whenever you want to save the model, passing it the session and path of the checkpoint file:\n",
    "[...]\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\")\n",
    "[...]\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0: # checkpoint every 100 epochs\n",
    "            save_path = saver.save(sess, \"/tmp/my_model.ckpt\")\n",
    "        sess.run(training_op)\n",
    "    best_theta = theta.eval()\n",
    "    save_path = saver.save(sess, \"/tmp/my_model_final.ckpt\")\n",
    "# Restoring a model is just as easy: you create a Saver at the end of the construction phase just like before,\n",
    "# but then at the beginning of the execution phase, instead of initializing the variables using the init node,\n",
    "# you call the restore() method of the Saver object:\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"/tmp/my_model_final.ckpt\")\n",
    "    [...]\n",
    "# By default, the save() method also saves the structure of the graph in a second file with the same name\n",
    "# plus a .meta extension. You can load this graph structure using tf.train.import_meta_graph() . This\n",
    "# adds the graph to the default graph, and returns a Saver instance that you can then use to restore the\n",
    "# graph’s state (i.e., the variable values):\n",
    "saver = tf.train.import_meta_graph(\"/tmp/my_model_final.ckpt.meta\")\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"/tmp/my_model_final.ckpt\")\n",
    "    [...]\n",
    "#     This allows you to fully restore a saved model, including both the graph structure and the variable values,\n",
    "# without having to search for the code that built it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the Graph and Training Curves Using TensorBoard\n",
    "\n",
    "# TensorBoard. If you feed it some training stats, it will display nice interactive visualizations of\n",
    "# these stats in your web browser (e.g., learning curves).\n",
    "# The first step is to tweak your program a bit so it writes the graph definition and some training stats — for\n",
    "# example, the training error (MSE) — to a log directory that TensorBoard will read from. You need to use\n",
    "# a different log directory every time you run your program, or else TensorBoard will merge stats from\n",
    "# different runs, which will mess up the visualizations.\n",
    "from datetime import datetime\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "# Next, add the following code at the very end of the construction phase:\n",
    "mse_summary = tf.summary.scalar('MSE', mse)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first line creates a node in the graph that will evaluate the MSE value and write it to a TensorBoard-\n",
    "# compatible binary log string called a summary. The second line creates a FileWriter that you will use\n",
    "# to write summaries to logfiles in the log directory. The first parameter indicates the path of the log\n",
    "# directory (in this case something like tf_logs/run-20160906091959/, relative to the current directory).\n",
    "# The second (optional) parameter is the graph you want to visualize. Upon creation, the FileWriter\n",
    "# creates the log directory if it does not already exist (and its parent directories if needed), and writes the\n",
    "# graph definition in a binary logfile called an events file.\n",
    "# Next you need to update the execution phase to evaluate the mse_summary node regularly during training\n",
    "# (e.g., every 10 mini-batches). This will output a summary that you can then write to the events file using\n",
    "# the file_writer . Here is the updated code:\n",
    "[...]\n",
    "for batch_index in range(n_batches):\n",
    "    X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "    if batch_index % 10 == 0:\n",
    "        summary_str = mse_summary.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        step = epoch * n_batches + batch_index\n",
    "        file_writer.add_summary(summary_str, step)\n",
    "    sess.run(training_op, feed_dict={X: X_batch, y: y_batch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, you want to close the FileWriter at the end of the program:\n",
    "file_writer.close()\n",
    "# Now run this program: it will create the log directory and write an events file in this directory, containing\n",
    "# both the graph definition and the MSE values. Open up a shell and go to your working directory, then type\n",
    "# ls -l tf_logs/run* to list the contents of the log directory:\n",
    "$ cd $ML_PATH\n",
    "# Your ML working directory (e.g., $HOME/ml)\n",
    "$ ls -l tf_logs/run*\n",
    "# total 40\n",
    "# -rw-r--r-- 1 ageron staff 18620 Sep 6 11:10 events.out.tfevents.1472553182.mymac\n",
    "# If you run the program a second time, you should see a second directory in the tf_logs/ directory:\n",
    "$ ls -l tf_logs/\n",
    "# total 0\n",
    "# drwxr-xr-x 3 ageron\n",
    "# drwxr-xr-x 3 ageron\n",
    "# staff\n",
    "# staff\n",
    "# 102 Sep\n",
    "# 102 Sep\n",
    "# 6 10:07 run-20160906091959\n",
    "# 6 10:22 run-20160906092202\n",
    "# Great! Now it’s time to fire up the TensorBoard server. You need to activate your virtualenv environment\n",
    "# if you created one, then start the server by running the tensorboard command, pointing it to the root log\n",
    "# directory. This starts the TensorBoard web server, listening on port 6006 (which is “goog” written upside\n",
    "# down):\n",
    "$ source env/bin/activate\n",
    "$ tensorboard --logdir tf_logs/\n",
    "Starting TensorBoard on port 6006\n",
    "(You can navigate to http://0.0.0.0:6006)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
